# Endpoint padrao do Ollama
#langchain4j.ollama.chat-model.base-url=http://localhost:11434

# Nome exato do modelo (deve ser o mesmo que aparece em 'ollama list')
#langchain4j.ollama.chat-model.model-name=deepseek-r1:8b

# Configuraçoes de parâmetros (opcional)
#langchain4j.ollama.chat-model.temperature=0.7
#langchain4j.ollama.chat-model.top-p=1.0

# IMPORTANTE: Aumentar o timeout para modelos grandes (20b+)
# O valor está em milissegundos (ex: 60 segundos)
#langchain4j.ollama.chat-model.timeout=PT60S

#langchain4j.google-ai.model-name=gemini-1.5-flash
# O modelo 'flash' é extremamente rápido e ideal para testes.
# Use 'gemini-1.5-pro' para tarefas mais complexas.